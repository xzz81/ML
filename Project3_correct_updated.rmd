---
itle: "Project3"
ououtput:
  pdf_document:
    latex_engine: xelatex
header-includes:
- \usepackage{geometry}
- \usepackage{setspace}
- \geometry{left=2.54cm,right=2.54cm,top=2.54cm,bottom=2.54cm}
- \setstretch{1.5}
output: pdf_document
fontsize: 12pt
---

```{r, echo=FALSE, message=FALSE, warning=FALSE,message=FALSE}
# Load necessary library
library(dplyr)
library(ggplot2)
library(gridExtra)
library(skimr)
library(gmodels)
library(GGally)
library(MLmetrics)
library(rpart)
library(rpart.plot)
library(caret)
library(class)
library(MASS)
library(neuralnet)
library(nnet)
library(knitr)

```

```{r, echo=FALSE, results='hide'}
# Load the data for drug consumption
group_23 <- read.csv('group_23.csv')
# Get rid of the fiction data
data <- group_23[group_23$Semer == 'CL0',]
```

# Introduction

In recent years, more and more drugs have begun to spread across various age groups. The intersection of personality psychology and substance use has garnered significant interest within the scientific community. This report collects 1885 respondents with 12 attributes, including education level, age, gender, country of residence, and ethnicity, providing a holistic view of each respondent NEO-FFI-R, encompassing five major dimensions of human personality (Neuroticism, Extraversion, Openness to Experience, Agreeableness, and Conscientiousness), alongside BIS-11 and ImpSSscales that measure impulsivity and sensation seeking.

The main aim of this report is to find the relationships between personality traits, demographic factors, and drug consumption patterns.

This report is structured as follows: The first section sets out the context of the study and cleaning the data. The second section makes Explanatory Data Analysis (EDA) and model building with 5 classification methods, including knearest neighbours, linear/quadratic discriminant analysis, tree-based methods, support vector machines, neural networks. The third section shows the 'best' model and results.Finally, the discussion section contextualizes these findings within the broaderfield of psychology and suggesting avenues for future research.

# Data Processing

Based on the dataset, we found that most respondents believe they belong to Group 'CL0' . Therefore, we conducted our analysis based on the data from the 'Semer' that only included Group 'CL0'. Additionally, based on the general distribution across the various groups for 'Amyl', we have divided the seven groups into four categories, as follows: 'Never Used' : CL0, 'Used Over a Decade Ago' : CL1, 'Used in Last Decade' : CL2, 'Used in Last Year to Day' : (CL3, CL4, CL5, CL6). Finally, to facilitate the creation of classification models, we have factorized these groups.

```{r echo=FALSE, fig.cap="Figure 1",warning=FALSE,message=FALSE}
library(knitr)
library(kableExtra)
amyl_usage <- data.frame(
  Amyl_grouped = c("Never Used", "Used Over a Decade Ago", "Used in Last Decade", "Used in Last Year"),
  Count = c(907, 151, 152, 103)
)

knitr::kable(amyl_usage, caption = "Distribution of amyl nitrite usage among respondents")

```

```{r, echo=FALSE, results='hide'}
# Check missing values
missing_values <- colSums(is.na(data))
missing_values
```

```{r, echo=FALSE,results='hide'}
grouped_data <- data %>%
  mutate(Amyl_grouped = case_when(
    Amyl == "CL0" ~ "Never Used",
    Amyl == "CL1" ~ "Used Over a Decade Ago",
    Amyl == "CL2" ~ "Used in Last Decade",
    Amyl %in% c("CL3", "CL4", "CL5", "CL6") ~ "Used in Last Year to Day"
  ))

str(grouped_data)
```

```{r, echo=FALSE,results='hide'}
# Set the levels of the factor
grouped_data$Amyl_grouped <- factor(grouped_data$Amyl_grouped, 
                                     levels = c("Never Used", "Used Over a Decade Ago", 
                                                "Used in Last Decade", "Used in Last Year to Day"))

# Distribution of the 'Amyl_grouped' categories
table(grouped_data$Amyl_grouped)
```

```{r,echo=FALSE,results='hide'}
set.seed(12345)
n <- nrow(grouped_data)
idx1 <- sample(c(1:n), floor(0.70 * n))
idx2 <- sample(c(1:n)[-idx1], floor(0.15 * n))
idx3 <- setdiff(c(1:n), c(idx1, idx2))
training_data <- grouped_data[idx1, ]
validation_data <- grouped_data[idx2, ]
testing_data <- grouped_data[idx3, ]
```

```{r,echo=FALSE,results='hide'}
str(training_data)
str(validation_data)
str(testing_data)
```

```{r,echo=FALSE,results='hide'}
grouped_counts <- training_data %>%
  dplyr::group_by(Amyl_grouped) %>%
  dplyr::summarise(Count = n())


grouped_counts
```

# Explanatory Data Analysis

```{r, echo=FALSE, results='hide'}
# Make a copy of the DataFrame
training_data.copy <- training_data
```

```{r, echo=FALSE, fig.cap="Distribution of Drug Use Categories (left) and Drug Consumption by Age (right).", fig.align="center", fig.align="center", fig.width=6, fig.height=3}
theme1 <- theme(
  axis.text.x = element_text(angle = 15, hjust = 1, size = 6),
  plot.title = element_text(size = 10),
  axis.text.y = element_text(size = 8)
)
# Bar plot for the updated 'Amyl_grouped' categories
g1 <- ggplot(training_data.copy, aes(x = Amyl_grouped)) + 
  geom_bar() + theme1


training_data.copy$Age_Category <- cut(
  training_data.copy$Age,
  breaks = c(-Inf, -0.95197, -0.07854, 0.49788, 1.09449, 1.82213, Inf),
  labels = c("18-24", "25-34", "35-44", "45-54", "55-64", "65+")
  )

g2 <-ggplot(training_data.copy, aes(x = Age_Category, fill = Amyl_grouped)) + 
  geom_bar(color = "white") +
  labs(x = "Age Group", y = "Count") + 
  theme1

grid.arrange(g1, g2, ncol = 2, widths = c(1, 1.75))
```

```{r, echo=FALSE, fig.cap= "Drug Consumption by Gender (left) and Age (right).", fig.align="center", fig.align="center", fig.width=6, fig.height=3}
g3 <- ggplot(training_data.copy, aes(x = as.factor(
  ifelse(Gender == 0.48246, "Female", "Male")), fill = Amyl_grouped)) + 
  geom_bar(color = "white", show.legend = FALSE) +
  labs(x = "Gender", y = "Count") + 
  theme1

training_data.copy$Edu_Category <- cut(
  training_data.copy$Education,
  breaks = c(-Inf, -2.43591, -1.73790, -1.43719, -1.22751, 
             -0.61113, -0.05921, 0.45468, 1.16365, Inf),
  labels = c("Left school before 16 years", "Left school at 16 years", 
             "Left school at 17 years", "Left school at 18 years", 
             "Some college or university, no certificate or degree",
             "Professional certificate/ diploma", 
             "University degree", "Masters degree", "Doctorate degree")
  )

g4 <- ggplot(training_data.copy, aes(x = Edu_Category, fill = Amyl_grouped)) + 
  geom_bar(colour = "white") +
  theme1

grid.arrange(g3, g4, ncol = 2, widths = c(1, 1.75))
```

```{r, echo=FALSE, fig.cap= "Country Distribution (left) and Ethnicity Distribution (right) Across Drug Consumption Categories.", fig.align="center", fig.align="center", fig.width=6, fig.height=3}
training_data.copy$Country_Category <- cut(
  training_data.copy$Country,
  breaks = c(-Inf, -0.57009, -0.46841, -0.28519, -0.09765, 0.21128, 0.24923, Inf),
  labels = c("USA", "New Zealand", "Other", "Australia", "Republic of Ireland", "Canada","UK")
  ) 

g5 <- ggplot(training_data.copy, aes(x = Country_Category, fill = Amyl_grouped)) + 
  geom_bar(colour = "white", show.legend = FALSE) +
  theme1

training_data.copy$Ethnicity_Category <- cut(
  training_data.copy$Ethnicity,
  breaks = c(-Inf, -1.10702, -0.50212, -0.31685, -0.22166, 0.11440, 0.12600, Inf),
  labels = c("Black", "Mixed-White/Black", "White", "Asian", 
             "Mixed-White/Asian", "Other", "Mixed-Black/Asian" )
  )

g6 <- ggplot(training_data.copy, aes(x = Ethnicity_Category, fill = Amyl_grouped)) + 
  geom_bar(colour = "white") +
  theme1
  
grid.arrange(g5, g6, ncol = 2, widths = c(1, 1.75))
```

```{r, echo=FALSE, fig.cap= "Drug last used time v.s. Personality Measures.", fig.align="center"}
theme2 <- theme(legend.position="none",
  axis.text.x = element_text(angle = 15, hjust = 1, size = 6),
  plot.title = element_text(size = 10),
  axis.text.y = element_text(size = 8))
  
cross_table_nscore <- table(training_data$Amyl_grouped, training_data$Nscore)
cross_table_df_nscore <- as.data.frame(cross_table_nscore)
names(cross_table_df_nscore) <- c("drug_last_use_time", "nscore", "frequency")
nscore<-ggplot(cross_table_df_nscore, aes(x = drug_last_use_time, y = frequency, fill = nscore)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Drug last use time vs Nscore",
       x = "Drug last use time", y = "Frequency") +
  theme2

cross_table_escore <- table(training_data$Amyl_grouped, training_data$Escore)
cross_table_df_escore <- as.data.frame(cross_table_escore)
names(cross_table_df_escore) <- c("drug_last_use_time", "escore", "frequency")
escore<-ggplot(cross_table_df_escore, aes(x = drug_last_use_time, y = frequency, fill = escore )) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Drug last use time vs Escore",
       x = "Drug last use time", y = "Frequency") +
  theme2

cross_table_oscore <- table(training_data$Amyl_grouped, training_data$Oscore)
cross_table_df_oscore <- as.data.frame(cross_table_oscore)
names(cross_table_df_oscore) <- c("drug_last_use_time", "oscore", "frequency")
oscore<-ggplot(cross_table_df_oscore, aes(x = drug_last_use_time, y = frequency, fill = oscore )) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Drug last use time vs Oscore",
       x = "Drug last use time", y = "Frequency") +
  theme2

cross_table_ascore <- table(training_data$Amyl_grouped, training_data$Ascore)
cross_table_df_ascore <- as.data.frame(cross_table_ascore)
names(cross_table_df_ascore) <- c("drug_last_use_time", "ascore", "frequency")
ascore<-ggplot(cross_table_df_ascore, aes(x = drug_last_use_time, y = frequency, fill = ascore )) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Drug last use time vs Ascore",
       x = "Drug last use time", y = "Frequency") +
  theme2

cross_table_cscore <- table(training_data$Amyl_grouped, training_data$Cscore)
cross_table_df_cscore <- as.data.frame(cross_table_cscore)
names(cross_table_df_cscore) <- c("drug_last_use_time", "cscore", "frequency")
cscore<-ggplot(cross_table_df_cscore, aes(x = drug_last_use_time, y = frequency, fill = cscore )) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Drug last use time vs Cscore",
       x = "Drug last use time", y = "Frequency") +
  theme2

cross_table_impulsive <- table(training_data$Amyl_grouped, training_data$Impulsive)
cross_table_df_impulsive <- as.data.frame(cross_table_impulsive)
names(cross_table_df_impulsive) <- c("drug_last_use_time", "impulsive", "frequency")
impulsive<-ggplot(cross_table_df_impulsive, aes(x = drug_last_use_time, y = frequency, fill = impulsive  )) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Drug last use time vs Impulsive",
       x = "Drug last use time", y = "Frequency") +
  theme2

cross_table_ss <- table(training_data$Amyl_grouped, training_data$SS)
cross_table_df_ss <- as.data.frame(cross_table_ss)
names(cross_table_df_ss) <- c("drug_last_use_time", "ss", "frequency")
ss<-ggplot(cross_table_df_ss, aes(x = drug_last_use_time, y = frequency, fill = ss )) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Drug last use time vs SS",
       x = "Drug last use time", y = "Frequency") +
  theme2
```

```{r echo=FALSE, fig.cap= "Drug last used time v.s. Personality Measures (Nscore, Escore, Oscore, Ascore).", fig.align="center"}
grid.arrange(nscore,escore,oscore,ascore,ncol=2)
```

```{r, echo=FALSE, fig.cap= "Drug last used time v.s. Personality Measures(Cscore, Impulsive, SS).", fig.align="center"}
grid.arrange(cscore,impulsive,ss,ncol=2)
```

# Classification Methods

## K-nearest Neighbours(KNN)

```{r}
#install.packages('kknn')
library(kknn)
#install.packages("caret")
library(caret)
library(dplyr)
#install.packages('class')
library(class)
```

```{r}
# Create an index for the training set based on ID
trainIndex <- createDataPartition(grouped_data$ID, p = .70, list = FALSE, times = 1)

# Create training set
trainSet <- grouped_data[trainIndex, ]

# Create a temporary dataset without the training data
tempSet <-  grouped_data[-trainIndex, ]

#split the remaining data into validation and test sets based on ID
validIndex <- createDataPartition(tempSet$ID, p = .50, list = FALSE, times = 1)

# Create validation and test sets
validSet <- tempSet[validIndex, ]
testSet <- tempSet[-validIndex, ]
```

```{r}
# deleate useless varibales
trainSet_scale <- trainSet[, !(colnames(trainSet) %in% c("ID", "Semer" ,'Amyl' ,'Amyl_grouped'))]

validSet_scale<- validSet[, !(colnames(validSet) %in% c("ID", "Semer" ,'Amyl' ,'Amyl_grouped'))]

testSet_scale<- testSet[, !(colnames(validSet) %in% c("ID", "Semer" ,'Amyl' ,'Amyl_grouped'))]
```

```{r}
#set original parameters
k <- c(1:20)
valid.pred = c()
valid.corr = c()

#begin the iteration to find the best parameter
for (i in k){
  valid.pred <- knn(trainSet_scale, validSet_scale, trainSet[,16] ,k =i)
  valid.corr[i] <- mean(validSet[,16] == valid.pred)
}

k.opt <- which.max(valid.corr)
plot(c(1:20), valid.corr, type="b", ylab="validation correct classification rate")
```

```{r}
#prediction with the trained model 
test.pred <- knn(trainSet_scale, testSet_scale, trainSet[,16] ,k =k.opt)

# created confusion matrix
confusionMatrix(test.pred,as.factor(testSet[,16]))
```
## Linear/Quadratic Discriminant Analysis

The linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) are linear and non-linear Fisher's Discriminant Analysis method. LDA optimizing the direction of separation among classes by finding linear functions of explanatory data with assumption of identical covariance matrices regardless of category. QDA is employed when equal variances assumption is violated, and it uses quadratic boundary of the predictors to distinguish between categories.

```{r, echo=FALSE, results='hide'}
# Prepare datasets for SVM method and also for LDA/QDA method
svm_data<-subset(training_data,select = -c(ID,Amyl,Semer))
svm_val<-subset(validation_data,select = -c(ID,Amyl,Semer))
svm_test<-subset(testing_data,select = -c(ID,Amyl,Semer))
```

```{r, echo=FALSE, results='hide'}
str(svm_data)
str(svm_val)
str(svm_test)
```

```{r, echo=FALSE, results='hide'}
library(MASS)  # For lda and qda functions
```

```{r}
# Fit a Linear Discriminant Analysis (LDA) model
lda_model <- lda(Amyl_grouped ~ ., data = svm_data)
# Fit a Quadratic Discriminant Analysis (QDA) model
qda_model <- qda(Amyl_grouped ~ ., data = svm_data)
```

```{r, echo=FALSE, results='hide'}
# Training, Validation and Testing predictions and accuracy with LDA
lda_training_pred <- predict(lda_model, training_data)$class
lda_training_acc <- mean(lda_training_pred == training_data$Amyl_grouped)
cat("Actual Training Accuracy for LDA:", lda_training_acc, "\n")

lda_val_pred <- predict(lda_model, svm_val)$class
lda_val_acc <- mean(lda_val_pred == svm_val$Amyl_grouped)
cat("Validation Accuracy for LDA:", lda_val_acc, "\n")

lda_test_pred <- predict(lda_model, svm_test)$class
lda_test_acc <- mean(lda_test_pred == svm_test$Amyl_grouped)
cat("Test Accuracy for LDA:", lda_test_acc, "\n")

# Training, Validation and Testing predictions and accuracy with QDA
qda_training_pred <- predict(qda_model, training_data)$class
qda_training_acc <- mean(qda_training_pred == training_data$Amyl_grouped)
cat("Actual Training Accuracy for QDA:", qda_training_acc, "\n")

qda_val_pred <- predict(qda_model, svm_val)$class
qda_val_acc <- mean(qda_val_pred == svm_val$Amyl_grouped)
cat("Validation Accuracy for QDA:", qda_val_acc, "\n")

qda_test_pred <- predict(qda_model, svm_test)$class
qda_test_acc <- mean(qda_test_pred == svm_test$Amyl_grouped)
cat("Test Accuracy for QDA:", qda_test_acc, "\n")
```

```{r, echo=FALSE, fig.cap="Figure",fig.align="center"}
library(knitr)
library(kableExtra)

# Accuracy values obtained for LDA and QDA models
accuracy_data_lda_qda <- data.frame(
  Model_Type = c("LDA", "QDA"),
  Training_Accuracy = c(lda_training_acc, qda_training_acc),
  Validation_Accuracy = c(lda_val_acc, qda_val_acc),
  Testing_Accuracy = c(lda_test_acc, qda_test_acc)
)

knitr::kable(accuracy_data_lda_qda, caption = "Training, Validation, and Testing Accuracy for LDA and QDA Models")
```

From the accuracy tale for LDA and QDA models, the LDA model performs better than the QDA, even though the QDA model offers more flexible boundaries:
- the LDA model gives consistent performance across training, validation, and test sets, while
- in the QDA model, there is a significant drop from the tarining to validation and test sets in accuracy, underscoring potential overfitting and less robust predictive ability.

However, the performance of balanced accuracy contrasts with the results of the accuracy analyses. Across all categories, QDA showed higher balanced accuracy compared to LDA.

```{r, echo=FALSE, results='hide'}
# Confusion Matrix for LDA
conf_matrix_lda <- confusionMatrix(as.factor(lda_test_pred), as.factor(testing_data$Amyl_grouped))
print("Confusion Matrix for LDA:")
print(conf_matrix_lda)

# Confusion Matrix for QDA
conf_matrix_qda <- confusionMatrix(as.factor(qda_test_pred), as.factor(testing_data$Amyl_grouped))
print("Confusion Matrix for QDA:")
print(conf_matrix_qda)
```

```{r, echo=FALSE, fig.cap="Figure",fig.align="center"}
library(knitr)
library(kableExtra)

# Balanced accuracy values obtained for LDA and QDA models of each class
balanced_accuracy_lda_qda <- data.frame(
  Class = c("Never Used", "Used Over a Decade Ago", "Used in Last Decade", "Used in Last Year to Day"),
  LDA_Balanced_Accuracy = c(0.49409, 0.507399, 0.5000, 0.4944),
  QDA_Balanced_Accuracy = c(0.6292, 0.5804, 0.60069, 0.50908)
)

knitr::kable(balanced_accuracy_lda_qda, caption = "Balanced Accuracy for Each Class for LDA and QDA Models")
```
Hence, the overall performance of the LDA and QDA models for drug consumption dataset do not perform well in generalization, as the relatively low balanced accuracy values from the confusion matrix across all classes, suggesting alternative classification methods should be considered.

## Support Vector Machines(SVM)

Support Vector Machines (SVMs) is a maximum margin classification algorithm, handling both linear and nonliear data through kernelisation. Unlike LDA/QDA derive decision boundaries as the result of a probabilistic framework whereas SVM constructs decision boundaries by identifying the hyperplane that maximizes the margin between the support vectors of different classes.

```{r, echo=FALSE, results='hide'}
# Load the e1071 package for SVM
library(e1071)
```

To apply support vector machines technique to predict the grouped categories of Amyl nitrite usage, various kernels (polynomial with degrees 3, radial basis function, and sigmoid respectively) are employed to train 4 distinct SVM models.

```{r}
# Train SVM model with different kernel
model_linear <- svm(training_data$Amyl_grouped~ . , svm_data, 
                    type = "C-classification", kernel = "linear")
model_poly3 <- svm(training_data$Amyl_grouped ~ ., data = svm_data, 
                   type = "C-classification", kernel = "polynomial", degree = 3)
model_rbf <- svm(training_data$Amyl_grouped ~ ., data = svm_data, 
                 type = "C-classification", kernel = "radial")
model_sigmoid <- svm(training_data$Amyl_grouped ~ ., data = 
                       svm_data, type = "C-classification", kernel = "sigmoid")
```

```{r, echo=FALSE, results='hide'}
# Training, Validation and Testing predictions and accuracy 
# with linear kernel
training_pred_linear <- predict(model_linear, svm_data)
acc_actual_linear <- mean(training_pred_linear == training_data$Amyl_grouped)
cat("Actual Training Accuracy for Linear Kernel:", 
            acc_actual_linear, "\n")

svm_pred_val_linear <- predict(model_linear, svm_val)
acc_val_linear <- mean(svm_pred_val_linear == validation_data$Amyl_grouped)
cat("Validation Accuracy for Linear Kernel:", acc_val_linear, "\n")

svm_pred_test_linear <- predict(model_linear, svm_test)
acc_test_linear <- mean(svm_pred_test_linear == testing_data$Amyl_grouped)
cat("Test Accuracy for Linear Kernel:", acc_test_linear, "\n")

# Training, Validation and Testing predictions and accuracy with Polynomial Kernel with Degree 3
training_pred_poly3 <- predict(model_poly3, svm_data)
# Calculate the accuracy by comparing the predictions with the actual values
acc_actual_poly3 <- mean(training_pred_poly3 == training_data$Amyl_grouped)
cat("Actual Training Accuracy for Polynomial Kernel Degree 3:", acc_actual_poly3, "\n")

svm_pred_val_poly3 <- predict(model_poly3, svm_val)
# Evaluate the performance of the model on the validation set
acc_val_poly3 <- mean(svm_pred_val_poly3 == validation_data$Amyl_grouped)
cat("Validation Accuracy for Polynomial Kernel Degree 3:", acc_val_poly3, "\n")

svm_pred_test_poly3 <- predict(model_poly3, svm_test)
# Evaluate the performance of the model on the testing set
acc_test_poly3 <- mean(svm_pred_test_poly3 == testing_data$Amyl_grouped)
cat("Test Accuracy for Polynomial Kernel Degree 3:", acc_test_poly3, "\n")

# Training, Validation and Testing predictions and accuracy with RBF Kernel
training_pred_rbf <- predict(model_rbf, svm_data)
# Calculate the accuracy by comparing the predictions with the actual values
acc_actual_rbf <- mean(training_pred_rbf == training_data$Amyl_grouped)
cat("Actual Training Accuracy for RBF Kernel:", acc_actual_rbf, "\n")

svm_pred_val_rbf <- predict(model_rbf, svm_val)
acc_val_rbf <- mean(svm_pred_val_rbf == validation_data$Amyl_grouped)
cat("Validation Accuracy for RBF Kernel:", acc_val_rbf, "\n")

svm_pred_test_rbf <- predict(model_rbf, svm_test)
acc_test_rbf <- mean(svm_pred_test_rbf == testing_data$Amyl_grouped)
cat("Test Accuracy for RBF Kernel:", acc_test_rbf, "\n")

# Training, Validation and Testing predictions and accuracy with Sigmoid Kernel
training_pred_sigmoid <- predict(model_sigmoid, svm_data)
acc_actual_sigmoid <- mean(training_pred_sigmoid == training_data$Amyl_grouped)
cat("Actual Training Accuracy for Sigmoid Kernel:", acc_actual_sigmoid, "\n")

svm_pred_val_sigmoid <- predict(model_sigmoid, svm_val)
acc_val_sigmoid <- mean(svm_pred_val_sigmoid == validation_data$Amyl_grouped)
cat("Validation Accuracy for Sigmoid Kernel:", acc_val_sigmoid, "\n")

svm_pred_test_sigmoid <- predict(model_sigmoid, svm_test)
acc_test_sigmoid <- mean(svm_pred_test_sigmoid == testing_data$Amyl_grouped)
cat("Test Accuracy for Sigmoid Kernel:", acc_test_sigmoid, "\n")
```

```{r, echo=FALSE, fig.cap="Figure",fig.align="center"}
library(knitr)
library(kableExtra)

# Accuracy values obtained for SVM models
svm_accuracy <- data.frame(
  Kernel_Type = c("Linear", "Polynomial Degree 3", "RBF", "Sigmoid"),
  Training_Accuracy = c(acc_actual_linear, acc_actual_poly3, acc_actual_rbf, acc_actual_sigmoid),
  Validation_Accuracy = c(acc_val_linear, acc_val_poly3, acc_val_rbf, acc_val_sigmoid),
  Testing_Accuracy = c(acc_test_linear, acc_test_poly3, acc_test_rbf, acc_test_sigmoid)
)

knitr::kable(svm_accuracy, caption = "Training, Validation, and Testing Accuracy for SVM Models")
```
The output of the SVM models with various kernels demonstrates that:

- The performance of the linear model appears to be consistent across training, validation and test datasets, indicating stable performance.

- This Polynomial Kernel with Degree 3 model showed higher accuracy on the training dataset compared to the other kernels. However, some overfitting may be present due to a slight decrease in accuracy on the validation and test datasets.

- The RBF kernel, similar to the linear kernel and polynomial kernel, shows consistent performance across datasets.

- The sigmoid kernel shows lower accuracy than the other kernels across datasets, indicating that it may not be suitable for this classification project.

Therefore, given the balance between accuracy, generalisation and model complexity, the RBF kernel offers outstanding adaptability and robustness across training, validation and testing data distributions. 

However, none of the kernels is performing strong performance, with the balanced accuracies equal or slightly below 0.5 across all classes as showed in the summary table:
```{r, echo=FALSE, results='hide'}
# Load the caret package
library(caret)
```

```{r, echo=FALSE, results='hide'}
# Confusion Matrix for Linear Kernel SVM
conf_matrix_linear <- confusionMatrix(as.factor(svm_pred_test_linear), as.factor(testing_data$Amyl_grouped))
print("Confusion Matrix for Linear Kernel SVM:")
print(conf_matrix_linear)

# Confusion Matrix for Polynomial Kernel SVM with Degree 3
conf_matrix_poly3 <- confusionMatrix(as.factor(svm_pred_test_poly3), as.factor(testing_data$Amyl_grouped))
print("Confusion Matrix for Polynomial Kernel SVM Degree 3:")
print(conf_matrix_poly3)

# Confusion Matrix for RBF Kernel SVM
conf_matrix_rbf <- confusionMatrix(as.factor(svm_pred_test_rbf), as.factor(testing_data$Amyl_grouped))
print("Confusion Matrix for RBF Kernel SVM:")
print(conf_matrix_rbf)

# Confusion Matrix for Sigmoid Kernel SVM
conf_matrix_sigmoid <- confusionMatrix(as.factor(svm_pred_test_sigmoid), as.factor(testing_data$Amyl_grouped))
print("Confusion Matrix for Sigmoid Kernel SVM:")
print(conf_matrix_sigmoid)
```

```{r, echo=FALSE, fig.cap="Figure",fig.align="center"}
library(knitr)
library(kableExtra)

# Balanced accuracy values obtained for SVM models of each class
balanced_accuracy_svm <- data.frame(
  Class = c("Never Used", "Used Over a Decade Ago", "Used in Last Decade", "Used in Last Year to Day"),
  Linear = c(0.5000, 0.5000, 0.5000, 0.5000),
  Poly_Degree_3 = c(0.49831, 0.498031, 0.512195, 0.496269),  
  RBF = c(0.5000, 0.5000, 0.5000, 0.5000),                 
  Sigmoid = c(0.45701, 0.493619, 0.483270, 0.4832))

knitr::kable(balanced_accuracy_svm, caption = "Balanced Accuracy for Each Class Across Different SVM Kernels")
```
This result may indicate that the drug consumption by time does not exhibit strong linear or simple non-linear separability, model tuning should be considered for next step by hyperparameter optimization, or that SVM may not be the optimal classifier choice for solving this classification problem without further feature engineering or consideration of other more complex models.

## Nerual Network (NNs)
For classification method choosing, we use NNs method since it handles nonlinearity associated with the data well. Neural networks are inspired by the architecture of biological neural systems, where processing elements, analogous to neurons, form the core of the network. Each neuron receives one or more inputs, processes those inputs, and produces an output. The main components of neural network information processing include inputs, weights, summation functions (the weighted average of all input data entering the processing element (PE), transformation functions, and outputs.


For constructing the neural network model, we use '**Amyl_grouped**' as the response variable and other variables as predicted variables. And with hidden layer1, 3or 5, we find model with 5 hidden layers has the highest accuracy. This model 

```{r,echo=FALSE, results='hide'}
library(neuralnet)
library(nnet)
#neural network
neural_data<-subset(training_data,select = -c(ID,Amyl,Semer))
neural_val<-subset(validation_data,select = -c(ID,Amyl,Semer))
neural_test<-subset(testing_data,select = -c(ID,Amyl,Semer))
```

```{r,echo=FALSE, results='hide'}
#since our dataset has some character variables which will influence the neural 
#network model, we remove variable 'ID', 'Amyl' and 'Semer'
#neural network model
#The weights of the neural network are usually initialized randomly at the 
#beginning of training. Using set.seed ensures that the initial values of the 
#weights are the same each time the model is trained.
set.seed(1)
#For construct the neural network model, we use 'Amyl_grouped' as the response 
#variable and other variables as predicted variables. And with hidden layer1, 3
#or 5, we find model with 5 hidden layers has the highest accuracy. Since our
#task is to classify, we set non-linear output. We set threshold = 0.01, which 
#specifies the convergence criteria during model training. This means that when 
#the total error improvement of the model is less than 0.01, the training 
#process will stop.
neural_model <- neuralnet(Amyl_grouped ~ ., data = neural_data, hidden = c(5), 
                          linear.output = FALSE,threshold = 0.01)
neural_model$result.matrix
neural_model
```

```{r,echo=FALSE}
#training data performance we calculate accuracy and balanced accuracy of the 
#neural network model for training data
training_expalin<-subset(neural_data,select = -Amyl_grouped)
train_pred<-compute(neural_model,training_expalin)
net_result<-train_pred$net.result
levels <- levels(neural_data$Amyl_grouped)
predicted_class_indices <- apply(net_result, 1, which.max)
predicted_classes <- levels[predicted_class_indices]
actual_classes <- neural_data$Amyl_grouped
```

```{r,echo=FALSE,results='hide'}
#since our response variable is multivariate classification, we need confusion
#matrix
conf_matrix_train <- table(Predicted = predicted_classes, 
                           Actual = actual_classes)
print(conf_matrix_train)
accuracy_train <- sum(diag(conf_matrix_train)) / sum(conf_matrix_train)
print(accuracy_train)
recall_per_class_train <- diag(conf_matrix_train) / rowSums(conf_matrix_train)
balanced_accuracy_train <- mean(recall_per_class_train)
print(balanced_accuracy_train)
#confusion matrix plot
conf_matrix_train_df <- as.data.frame(as.table(conf_matrix_train))
ggplot(data = conf_matrix_train_df, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), vjust = 1) +  
  scale_fill_gradient(low = "white", high = "steelblue") +  
  theme_minimal() + 
  labs(title = "Confusion Matrix for Training Data", x = "Actual Class", 
       y = "Predicted Class")  
```

The table output indicates that:

-   The values along the main diagonal of the confusion matrix (from the bottom left to the top right) represent the number of instances that were correctly classified. For the "Never Used" category, there are 867 instances that were correctly classified, which is the largest number on the diagonal, indicating that the model performs best on this category.

-   The off-diagonal values represent the number of instances that were misclassified. For instance, out of the actual "Used Over a Decade Ago" instances, 48 were incorrectly classified as "Used in Last Year to Decade". These types of errors are known as "false negatives" in machine learning.

-   There is a large concentration of instances in the "Never Used" category, suggesting a skewed distribution of the dataset towards this category. The model's high accuracy in this category might be partly due to the over-representation of "Never Used" instances in the dataset, as the dataset shown.

However, the performance of this neural model concentrates on the category 'Never', indicating that this model might struggle to predict less when model predict categories such as 'Used in Last Decade'. In summary, this model just shows good performance on 'Never Used' compared other categories and there is room for improvement in handling imbalanced data and rare categories.
#from plot, we can find that the model is very good at identifying the category
```

```{r,echo=FALSE,results='hide'}
#"Never Used".
#validation This is same for validation data.
predictions_val <- compute(neural_model, neural_val[,c('Age','Gender',
                                                       'Education','Country',
                                                       'Ethnicity','Nscore',
                                                       'Escore','Oscore',
                                                       'Ascore','Cscore',
                                                       'Impulsive','SS')])
pred_values_val <- predictions_val$net.result
#accuracy
predicted_classes_val <- apply(pred_values_val, 1, function(x) which.max(x) - 1)  
#confusion matrix:the actual and predicted classes are passed as arguments.
conf_matrix_val <- table(Actual =neural_val$Amyl_grouped, 
                         Predicted = predicted_classes_val)
print(conf_matrix_val)
#we calculate accuracy by summing the diagonal elements of the confusion matrix 
#divided the total 
accuracy_val <- sum(diag(conf_matrix_val)) / sum(conf_matrix_val)
print(paste("Accuracy of validation:", accuracy_val))
#We find that there is subtle gaps between accuracy of model for training and 
#validation data, indicating this neural network model isn't overfit.
# rowsum
row_totals <- rowSums(conf_matrix_val)
# replace NA
recall_per_class_val <- rep(NA, length(row_totals))
valid_rows <- row_totals > 0
recall_per_class_val[valid_rows] <- diag(conf_matrix_val)[valid_rows] / row_totals[valid_rows]
balanced_accuracy_val <- mean(recall_per_class_val, na.rm = TRUE)

print(balanced_accuracy_val)
```

```{r,echo=FALSE,results='hide'}
#test This is same for test data. We apply the final neural model for the test 
#data
predictions <- neuralnet::compute(neural_model, neural_test[,c('Age','Gender','Education',
                                                    'Country','Ethnicity',
                                                    'Nscore','Escore','Oscore',
                                                    'Ascore','Cscore',
                                                    'Impulsive','SS')])
pred_values <- predictions$net.result
#accuracy
predicted_classes_test <- apply(pred_values, 1, function(x) which.max(x) - 1)  
#confusion matrix:the actual and predicted classes are passed as arguments.
conf_matrix_test <- table(Actual =neural_test$Amyl_grouped, 
                          Predicted = predicted_classes_test)
print(conf_matrix_test)

#we calculate accuracy by summing the diagonal elements of the confusion matrix 
#divided the total 
accuracy <- sum(diag(conf_matrix_test)) / sum(conf_matrix_test)
print(paste("Accuracy of test:", accuracy))
#The accuracy is 0.63
row_totals_test <- rowSums(conf_matrix_test)
recall_per_class_test <- rep(NA, length(row_totals_test))
valid_rows_test <- row_totals_test > 0
recall_per_class_test[valid_rows] <- diag(conf_matrix_test)[valid_rows_test] / row_totals_test[valid_rows_test]
balanced_accuracy_test <- mean(recall_per_class_test, na.rm = TRUE)
print(balanced_accuracy_test)
```

```{r}
str(training_data)
str(validation_data)
```

## Tree-based Method

```{r echo=FALSE}
# subset the data set
train_data<-subset(training_data,
  select=c(-Semer,-ID,-Amyl))
valid_data<-subset(validation_data,
  select=c(-Amyl,-Semer,-ID))
test_data<-subset(testing_data,
  select=c(-Amyl,-Semer,-ID))
```

```{r echo=FALSE}
# definite the function to find out the tree with best balance accuracy on test data set
find_best_cp <- function(full_tree, training_data, validation_data, testing_data, cp_start, cp_end, steps, tolerance) {
  cp_values <- seq(cp_start, cp_end, length.out = steps)
  best_accuracy <- 0
  best_cp <- 0
  best_tree <- NULL
  balanced_accuracy<-c()
  
  for(cp in cp_values) {
    # cut the tree
    pruned_tree <- prune(full_tree, cp = cp)  # construct training data confusion matrix
    # predict in training data set
    predictions_train <- predict(pruned_tree, training_data, type = "class")
    y_predic_train<-as.factor(predictions_train)
    y_actual_train<-as.factor(training_data$Amyl_grouped)
    conf_matrix_train <- confusionMatrix(y_predic_train,y_actual_train)
    
    #calculate accuracy
    recalls_train <- conf_matrix_train$byClass[,"Balanced Accuracy"]
    class_counts <- table(y_actual_train)
    accuracy_train<-conf_matrix_train$overall["Accuracy"]
    
    
    
    # predict in validation dataset
    valid_pred <- predict(pruned_tree, validation_data, type = "class")
    
    # calculate the weighted balanced accuracy for validation dataset
    predictions_validation <- predict(pruned_tree, validation_data, type = "class")
    y_predic_validation<-as.factor(predictions_validation)
    y_actual_validation<-as.factor(validation_data$Amyl_grouped)
    conf_matrix_validation <- confusionMatrix(y_predic_validation,y_actual_validation)
    accuracy_validation<-conf_matrix_validation$overall["Accuracy"]
    
    # the value of accuracy for validation
    if(accuracy_train <= (accuracy_validation + tolerance)|accuracy_validation>=accuracy_train) { 
      
      # predict on the test data 
      predictions_test <- predict(pruned_tree, testing_data, type = "class")
      
      # calculate the weighted_balanced_accuracy_test 
      y_predic_test<-as.factor(predictions_test)
      y_actual_test<-as.factor(testing_data$Amyl_grouped)
      conf_matrix_test <- confusionMatrix(y_predic_test,y_actual_test)
      recalls_test <- conf_matrix_test$byClass[,"Balanced Accuracy"]
      accuracy_test<- conf_matrix_test$overall["Accuracy"]
      
      
      
      # test whether is the best balanced accurancy 
      if(accuracy_test >= best_accuracy) {
        best_accuracy <- accuracy_test
        best_cp <- cp
        best_tree <- pruned_tree
        balanced_accuracy<-recalls_test
      }
    }
  }
  
  list(best_tree = best_tree, best_cp = best_cp, best_accuracy = best_accuracy ,balanced_accuracy=balanced_accuracy)
}
```

```{r echo=FALSE}
# construct full tree in training data set
set.seed(12345)
full_tree <- rpart(Amyl_grouped~., data=train_data, method="class", control=rpart.control(minsplit=2,minbucket=round(20/3),maxdepth = 30, cp=-1))
printcp(full_tree)

```

```{r}
plotcp(full_tree)
```

```{r echo=FALSE}
# find best cp for the tree we build up 
best_model <- find_best_cp(full_tree = full_tree, 
                           training_data = train_data, 
                           validation_data = valid_data,
                           testing_data = test_data, 
                           cp_start = 0.00640393, 
                           cp_end = 0, 
                           steps = 100,
                           tolerance = 0.06) 
```

```{r echo=FALSE}
print(best_model$best_cp)
print(best_model$best_accuracy)
print(best_model$balanced_accuracy)
```

```{r echo= FALSE}
# plot the best tree
best_tree<- prune(full_tree,cp = best_model$best_cp)
rpart.plot(best_tree,type=2,extra=4)
```

# Model Comparison 

# Conclusion 
